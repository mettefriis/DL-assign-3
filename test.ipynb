{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--z_dim Z_DIM] [--num_filters NUM_FILTERS]\n",
      "                             [--lr LR] [--batch_size BATCH_SIZE]\n",
      "                             [--data_dir DATA_DIR] [--epochs EPOCHS]\n",
      "                             [--seed SEED] [--num_workers NUM_WORKERS]\n",
      "                             [--log_dir LOG_DIR] [--progress_bar]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/Users/mette/Library/Jupyter/runtime/kernel-v31efd0ef299b64b2d4cc34e6c3057c7b3fb0c7039.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2022\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to conditions.\n",
    "#\n",
    "# Author: Deep Learning Course | Autumn 2022\n",
    "# Date Created: 2022-11-25\n",
    "################################################################################\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from mnist import mnist\n",
    "from cnn_encoder_decoder import CNNEncoder, CNNDecoder\n",
    "from utils import *\n",
    "\n",
    "\n",
    "class VAE(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, num_filters, z_dim, lr):\n",
    "        \"\"\"\n",
    "        PyTorch Lightning module that summarizes all components to train a VAE.\n",
    "        Inputs:\n",
    "            num_filters - Number of channels to use in a CNN encoder/decoder\n",
    "            z_dim - Dimensionality of latent space\n",
    "            lr - Learning rate to use for the optimizer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.encoder = CNNEncoder(z_dim=z_dim, num_filters=num_filters)\n",
    "        self.decoder = CNNDecoder(z_dim=z_dim, num_filters=num_filters)\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        \"\"\"\n",
    "        The forward function calculates the VAE-loss for a given batch of images.\n",
    "        Inputs:\n",
    "            imgs - Batch of images of shape [B,C,H,W].\n",
    "                   The input images are converted to 4-bit, i.e. integers between 0 and 15.\n",
    "        Ouptuts:\n",
    "            L_rec - The average reconstruction loss of the batch. Shape: single scalar\n",
    "            L_reg - The average regularization loss (KLD) of the batch. Shape: single scalar\n",
    "            bpd - The average bits per dimension metric of the batch.\n",
    "                  This is also the loss we train on. Shape: single scalar\n",
    "        \"\"\"\n",
    "\n",
    "        # Hints:\n",
    "        # - Implement the empty functions in utils.py before continuing\n",
    "        # - The forward run consists of encoding the images, sampling in\n",
    "        #   latent space, and decoding.\n",
    "        # - By default, torch.nn.functional.cross_entropy takes the mean accross\n",
    "        #   all axes. Do not forget to change the 'reduction' parameter to\n",
    "        #   make it consistent with the loss definition of the assignment.\n",
    "\n",
    "        #######################\n",
    "        # PUT YOUR CODE HERE  #\n",
    "        #######################\n",
    "        #save dimensions of imgs for later use in bpd:\n",
    "        \n",
    "        original_shape = imgs.shape \n",
    "\n",
    "        mu, log_std = self.encoder(imgs)\n",
    "        #print(\"mu shape:\", mu.shape)\n",
    "        #print(\"log_std shape:\", log_std.shape)\n",
    "        #print(\"mu:\", mu)\n",
    "        #print(\"log_std:\", log_std)\n",
    "\n",
    "        z=sample_reparameterize(mu, log_std)\n",
    "        recon_imgs = self.decoder(z) #reconstructed images\n",
    "\n",
    "        #modify shapes:\n",
    "        #recon_imgs = recon_imgs.view(recon_imgs.size(0), 16, -1)\n",
    "        #imgs = imgs.view(imgs.size(0), -1).long()\n",
    "        \n",
    "        imgs = (imgs // 15).long()\n",
    "        print(\"Class distribution in imgs:\", torch.bincount(imgs.view(-1)))\n",
    "\n",
    "        recon_imgs = recon_imgs.view(recon_imgs.size(0), 16, 28, 28)  # [B, C, H, W]\n",
    "        imgs = imgs.view(imgs.size(0), 28, 28)  # [B, H, W]\n",
    "        imgs = (imgs * 15).long()\n",
    "        \n",
    "        #print(recon_imgs.shape)\n",
    "        #print(imgs.shape)\n",
    "        #print(\"Unique values in imgs:\", imgs.unique())\n",
    "        \n",
    "        L_rec = F.cross_entropy(\n",
    "            recon_imgs,\n",
    "            imgs,\n",
    "            reduction='none'\n",
    "        )\n",
    "        L_rec=L_rec.sum(dim=[1,2]).mean(dim=0)\n",
    "        #print(L_rec.shape)\n",
    "  \n",
    "        #print(\"Type of L_rec:\", L_rec.shape)\n",
    "     \n",
    "        \n",
    "\n",
    "        L_reg = torch.mean(KLD(mu, log_std))\n",
    "\n",
    "\n",
    "        #we use elbo_to_bpd(elbo, img_shape) by forst estmating elbo\n",
    "        elbo= L_rec + L_reg #check here! + or -\n",
    "        img_shape=imgs.shape\n",
    "        #print(img_shape)\n",
    "        batch_size, channels, height, width = original_shape\n",
    "        recovered_img_shape = (batch_size, channels, height, width)\n",
    "\n",
    "        bpd = elbo_to_bpd(elbo, recovered_img_shape)\n",
    "   \n",
    "        #######################\n",
    "        # END OF YOUR CODE    #\n",
    "        #######################\n",
    "        return L_rec, L_reg, bpd\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Function for sampling a new batch of random images.\n",
    "        Inputs:\n",
    "            batch_size - Number of images to generate\n",
    "        Outputs:\n",
    "            x_samples - Sampled, 4-bit images. Shape: [B,C,H,W]\n",
    "        \"\"\"\n",
    "        #######################\n",
    "        # PUT YOUR CODE HERE  #\n",
    "        #######################\n",
    "        z = torch.randn(batch_size, self.hparams.z_dim, device=self.device) #sample from N(0, 1)\n",
    "        x_samples = self.decoder(z) #decode #[batch_size, channels, height, width].\n",
    "        x_samples = torch.clamp(x_samples, 0, 15).int() #limits values from 0 to 15 \n",
    "        #######################\n",
    "        # END OF YOUR CODE    #\n",
    "        #######################\n",
    "        return x_samples\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Create optimizer\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Make use of the forward function, and add logging statements\n",
    "        L_rec, L_reg, bpd = self.forward(batch[0])\n",
    "        self.log(\"train_reconstruction_loss\", L_rec, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_regularization_loss\", L_reg, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_ELBO\", L_rec + L_reg, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_bpd\", bpd, on_step=False, on_epoch=True)\n",
    "\n",
    "        return bpd\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Make use of the forward function, and add logging statements\n",
    "        L_rec, L_reg, bpd = self.forward(batch[0])\n",
    "        self.log(\"val_reconstruction_loss\", L_rec)\n",
    "        self.log(\"val_regularization_loss\", L_reg)\n",
    "        self.log(\"val_ELBO\", L_rec + L_reg)\n",
    "        self.log(\"val_bpd\", bpd)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Make use of the forward function, and add logging statements\n",
    "        L_rec, L_reg, bpd = self.forward(batch[0])\n",
    "        self.log(\"test_bpd\", bpd)\n",
    "\n",
    "\n",
    "class GenerateCallback(pl.Callback):\n",
    "\n",
    "    def __init__(self, batch_size=64, every_n_epochs=5, save_to_disk=False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            batch_size - Number of images to generate\n",
    "            every_n_epochs - Only save those images every N epochs (otherwise tensorboard gets quite large)\n",
    "            save_to_disk - If True, the samples and image means should be saved to disk as well.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.every_n_epochs = every_n_epochs\n",
    "        self.save_to_disk = save_to_disk\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        \"\"\"\n",
    "        This function is called after every epoch.\n",
    "        Call the save_and_sample function every N epochs.\n",
    "        \"\"\"\n",
    "        if (trainer.current_epoch+1) % self.every_n_epochs == 0:\n",
    "            self.sample_and_save(trainer, pl_module, trainer.current_epoch+1)\n",
    "\n",
    "    def sample_and_save(self, trainer, pl_module, epoch):\n",
    "        \"\"\"\n",
    "        Function that generates and save samples from the VAE.\n",
    "        The generated sample images should be added to TensorBoard and,\n",
    "        if self.save_to_disk is True, saved inside the logging directory.\n",
    "        Inputs:\n",
    "            trainer - The PyTorch Lightning \"Trainer\" object.\n",
    "            pl_module - The VAE model that is currently being trained.\n",
    "            epoch - The epoch number to use for TensorBoard logging and saving of the files.\n",
    "        \"\"\"\n",
    "        samples = pl_module.sample(self.batch_size)\n",
    "        samples = samples.float() / 15  # Converting 4-bit images to values between 0 and 1\n",
    "        grid = make_grid(samples, nrow=8, normalize=True, value_range=(0, 1), pad_value=0.5)\n",
    "        grid = grid.detach().cpu()\n",
    "        trainer.logger.experiment.add_image(\"Samples\", grid, global_step=epoch)\n",
    "        if self.save_to_disk:\n",
    "            save_image(grid,\n",
    "                        os.path.join(trainer.logger.log_dir, f\"epoch_{epoch}_samples.png\"))\n",
    "\n",
    "\n",
    "def train_vae(args):\n",
    "    \"\"\"\n",
    "    Function for training and testing a VAE model.\n",
    "    Inputs:\n",
    "        args - Namespace object from the argument parser\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(args.log_dir, exist_ok=True)\n",
    "    train_loader, val_loader, test_loader = mnist(batch_size=args.batch_size,\n",
    "                                                   num_workers=args.num_workers,\n",
    "                                                   root=args.data_dir)\n",
    "\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    gen_callback = GenerateCallback(save_to_disk=True)\n",
    "    save_callback = ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"val_bpd\")\n",
    "    trainer = pl.Trainer(default_root_dir=args.log_dir,\n",
    "                         accelerator=\"auto\",\n",
    "                         max_epochs=args.epochs,\n",
    "                         callbacks=[save_callback, gen_callback],\n",
    "                         enable_progress_bar=args.progress_bar)\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "    if not args.progress_bar:\n",
    "        print(\"[INFO] The progress bar has been suppressed. For updates on the training \" + \\\n",
    "              f\"progress, check the TensorBoard file at {trainer.logger.log_dir}. If you \" + \\\n",
    "              \"want to see the progress bar, use the argparse option \\\"progress_bar\\\".\\n\")\n",
    "\n",
    "    # Create model\n",
    "    pl.seed_everything(args.seed)  # To be reproducible\n",
    "    model = VAE(num_filters=args.num_filters,\n",
    "                z_dim=args.z_dim,\n",
    "                lr=args.lr)\n",
    "\n",
    "    # Training\n",
    "    gen_callback.sample_and_save(trainer, model, epoch=0)  # Initial sample\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # Testing\n",
    "    model = VAE.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "    test_result = trainer.test(model, dataloaders=test_loader, verbose=True)\n",
    "\n",
    "    # Manifold generation\n",
    "    if args.z_dim == 2:\n",
    "        img_grid = visualize_manifold(model.decoder)\n",
    "        save_image(img_grid,\n",
    "                   os.path.join(trainer.logger.log_dir, 'vae_manifold.png'),\n",
    "                   normalize=False)\n",
    "\n",
    "    return test_result\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Feel free to add more argument parameters\n",
    "    parser = argparse.ArgumentParser(\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "    # Model hyperparameters\n",
    "    parser.add_argument('--z_dim', default=20, type=int,\n",
    "                        help='Dimensionality of latent space')\n",
    "    parser.add_argument('--num_filters', default=32, type=int,\n",
    "                        help='Number of channels/filters to use in the CNN encoder/decoder.')\n",
    "\n",
    "    # Optimizer hyperparameters\n",
    "    parser.add_argument('--lr', default=1e-3, type=float,\n",
    "                        help='Learning rate to use')\n",
    "    parser.add_argument('--batch_size', default=128, type=int,\n",
    "                        help='Minibatch size')\n",
    "\n",
    "    # Other hyperparameters\n",
    "    parser.add_argument('--data_dir', default='../data/', type=str,\n",
    "                        help='Directory where to look for the data. For jobs on Lisa, this should be $TMPDIR.')\n",
    "    parser.add_argument('--epochs', default=80, type=int,\n",
    "                        help='Max number of epochs')\n",
    "    parser.add_argument('--seed', default=42, type=int,\n",
    "                        help='Seed to use for reproducing results')\n",
    "    parser.add_argument('--num_workers', default=4, type=int,\n",
    "                        help='Number of workers to use in the data loaders. To have a truly deterministic run, this has to be 0. ' + \\\n",
    "                             'For your assignment report, you can use multiple workers (e.g. 4) and do not have to set it to 0.')\n",
    "    parser.add_argument('--log_dir', default='VAE_logs', type=str,\n",
    "                        help='Directory where the PyTorch Lightning logs should be created.')\n",
    "    parser.add_argument('--progress_bar', action='store_true',\n",
    "                        help=('Use a progress bar indicator for interactive experimentation. '\n",
    "                              'Not to be used in conjuction with SLURM jobs'))\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    train_vae(args)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
